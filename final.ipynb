{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# IMPORTS\n",
    "#########\n",
    "import random as rand\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "from tsp_solver.greedy import solve_tsp as solve\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets import utils_np\n",
    "from graph_nets.demos import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# UTILS\n",
    "#########\n",
    "\n",
    "\n",
    "def create_random_graph(node_range=(5, 9), prob=0.25, weight_range=(1, 10)):\n",
    "    n_nodes = rand.randint(*node_range)\n",
    "\n",
    "    G = nx.complete_graph(n_nodes)\n",
    "    H = G.copy()\n",
    "    for u, v, w in G.edges(data=True):\n",
    "        H[u][v][\"weight\"] = rand.randint(*weight_range)\n",
    "\n",
    "        # u_deg, v_deg = H.degree(u), H.degree(v)\n",
    "        # if u_deg - 1 >= n_nodes / 2 and v_deg - 1 >= n_nodes / 2:\n",
    "        #     if rand.random() < prob:\n",
    "        #         H.remove_edge(u, v)\n",
    "\n",
    "    return H\n",
    "\n",
    "\n",
    "def solve_tsp(graph):\n",
    "    adj_matrix = nx.adjacency_matrix(graph)\n",
    "    hamil_path = solve(adj_matrix.todense().tolist())\n",
    "\n",
    "    path_edges = [(hamil_path[i], hamil_path[i + 1])\n",
    "                  for i in range(len(hamil_path) - 1)]\n",
    "    path_edges.append((hamil_path[-1], hamil_path[0]))\n",
    "\n",
    "    for u, v in graph.edges():\n",
    "        graph[u][v][\"solution\"] = int(\n",
    "            any({u, v}.issubset({src, targ}) for src, targ in path_edges))\n",
    "\n",
    "    solution_dict = {v: False for v in graph.nodes()}\n",
    "    for u, v in path_edges:\n",
    "        solution_dict[u] = True\n",
    "        solution_dict[v] = True\n",
    "\n",
    "    nx.set_node_attributes(graph, solution_dict, \"solution\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def to_one_hot(indices, max_value, axis=-1):\n",
    "    one_hot = np.eye(max_value)[indices]\n",
    "    if axis not in (-1, one_hot.ndim):\n",
    "        one_hot = np.moveaxis(one_hot, -1, axis)\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def graph_to_input_target(graph):\n",
    "    \"\"\"Returns 2 graphs with input and target feature vectors for training.\n",
    "    Args:\n",
    "    graph: An `nx.Graph` instance.\n",
    "    Returns:\n",
    "    The input `nx.Graph` instance.\n",
    "    The target `nx.Graph` instance.\n",
    "    Raises:\n",
    "    ValueError: unknown node type\n",
    "    \"\"\"\n",
    "\n",
    "    def create_feature(attr, fields):\n",
    "        return np.hstack([np.array(attr[field], dtype=float) for field in fields])\n",
    "\n",
    "    input_node_fields = (\"solution\",)\n",
    "    input_edge_fields = (\"weight\",)\n",
    "    target_node_fields = (\"solution\",)\n",
    "    target_edge_fields = (\"solution\",)\n",
    "\n",
    "    input_graph = graph.copy()\n",
    "    target_graph = graph.copy()\n",
    "\n",
    "    solution_length = 0\n",
    "    for node_index, node_feature in graph.nodes(data=True):\n",
    "        input_graph.add_node(\n",
    "            node_index, features=create_feature(node_feature, input_node_fields))\n",
    "        target_node = to_one_hot(\n",
    "            create_feature(node_feature, target_node_fields).astype(int), 2)[0]\n",
    "        target_graph.add_node(node_index, features=target_node)\n",
    "\n",
    "    for receiver, sender, features in graph.edges(data=True):\n",
    "        input_graph.add_edge(\n",
    "            sender, receiver, features=create_feature(features, input_edge_fields))\n",
    "        target_edge = to_one_hot(\n",
    "            create_feature(features, target_edge_fields).astype(int), 2)[0]\n",
    "        target_graph.add_edge(sender, receiver, features=target_edge)\n",
    "        solution_length += features[\"weight\"] * features[\"solution\"]\n",
    "\n",
    "    input_graph.graph[\"features\"] = np.array([0.0])\n",
    "    target_graph.graph[\"features\"] = np.array([solution_length], dtype=float)\n",
    "\n",
    "    # print(type(input_graph))\n",
    "    # print(input_graph.graph)\n",
    "    # print(type(target_graph))\n",
    "    # print(target_graph.graph)\n",
    "\n",
    "    return input_graph, target_graph\n",
    "\n",
    "\n",
    "def generate_networkx_graphs(num_graphs, node_range=(5, 9), prob=0.25, weight_range=(1, 10)):\n",
    "    \"\"\"Generate graphs for training.\n",
    "    Args:\n",
    "    num_graphs: number of graphs to generate\n",
    "    num_range: a 2-tuple with the [lower, upper) number of nodes per\n",
    "      graph\n",
    "    prob: the probability of removing an edge between any two nodes\n",
    "    weight_range: a 2-tuple with the [lower, upper) weight to randomly assign\n",
    "        to (non-removed) edges\n",
    "    Returns:\n",
    "    input_graphs: The list of input graphs.\n",
    "    target_graphs: The list of output graphs.\n",
    "    graphs: The list of generated graphs.\n",
    "    \"\"\"\n",
    "\n",
    "    input_graphs = []\n",
    "    target_graphs = []\n",
    "    graphs = []\n",
    "\n",
    "    for i in range(num_graphs):\n",
    "        graph = create_random_graph(node_range, prob, weight_range)\n",
    "        graph = solve_tsp(graph)\n",
    "        input_graph, target_graph = graph_to_input_target(graph)\n",
    "        input_graphs.append(input_graph)\n",
    "        target_graphs.append(target_graph)\n",
    "        graphs.append(graph)\n",
    "\n",
    "    return input_graphs, target_graphs, graphs\n",
    "\n",
    "\n",
    "#########\n",
    "# EXPORTS\n",
    "#########\n",
    "\n",
    "\n",
    "def create_placeholders(num_graphs):\n",
    "    input_graphs, target_graphs, _ = generate_networkx_graphs(num_graphs)\n",
    "    input_ph = utils_tf.placeholders_from_networkxs(input_graphs)\n",
    "    target_ph = utils_tf.placeholders_from_networkxs(target_graphs)\n",
    "    return input_ph, target_ph\n",
    "\n",
    "\n",
    "def make_all_runnable_in_session(*args):\n",
    "    \"\"\"Lets an iterable of TF graphs be output from a session as NP graphs.\"\"\"\n",
    "    return [utils_tf.make_runnable_in_session(a) for a in args]\n",
    "\n",
    "\n",
    "def create_feed_dict(num_graphs, input_ph, target_ph):\n",
    "    \"\"\"Creates placeholders for the model training and evaluation.\"\"\"\n",
    "\n",
    "    inputs, targets, raw_graphs = generate_networkx_graphs(num_graphs)\n",
    "    input_graphs = utils_np.networkxs_to_graphs_tuple(inputs)\n",
    "    target_graphs = utils_np.networkxs_to_graphs_tuple(targets)\n",
    "    feed_dict = {input_ph: input_graphs, target_ph: target_graphs}\n",
    "\n",
    "    return feed_dict, inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# TRAINING_HELPER (THAT CAN BE TUNED)\n",
    "#########\n",
    "def create_loss_ops(target_op, output_ops):\n",
    "    return [\n",
    "        tf.losses.softmax_cross_entropy(target_op.edges, output_op.edges)\n",
    "        for output_op in output_ops\n",
    "    ]\n",
    "\n",
    "\n",
    "def compute_accuracy(target, output, use_nodes=True, use_edges=False):\n",
    "    if not use_nodes and not use_edges:\n",
    "        raise ValueError(\"Nodes or edges (or both) must be used\")\n",
    "\n",
    "    tdds = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "    odds = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "\n",
    "    cs, ss = [], []\n",
    "    for td, od in zip(tdds, odds):\n",
    "\n",
    "        xe = np.argmax(td[\"edges\"], axis=-1)\n",
    "        ye = np.argmax(od[\"edges\"], axis=-1)\n",
    "\n",
    "        c = [xe == ye] if use_edges else []\n",
    "        c = np.concatenate(c, axis=0)\n",
    "\n",
    "        s = np.all(c)\n",
    "        cs.append(c)\n",
    "        ss.append(s)\n",
    "\n",
    "    correct = np.mean(np.concatenate(cs, axis=0))\n",
    "    solved = np.mean(np.stack(ss))\n",
    "\n",
    "    return correct, solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/roger/Library/Python/3.7/lib/python/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/roger/Library/Python/3.7/lib/python/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/roger/Library/Python/3.7/lib/python/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roger/Library/Python/3.7/lib/python/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# MODEL AND HYPERPARAMETERS SETUP\n",
    "#########\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 2\n",
    "rand = np.random.RandomState(seed=seed)\n",
    "\n",
    "# Model parameters; no. of message-passing steps\n",
    "num_processing_steps_tr = 10\n",
    "num_processing_steps_ge = 10\n",
    "\n",
    "# Data / training parameters\n",
    "num_training_iterations = 1000\n",
    "batch_size_tr = 100\n",
    "batch_size_ge = 32\n",
    "\n",
    "# Input and target placeholders\n",
    "input_ph, target_ph = create_placeholders(batch_size_tr)\n",
    "\n",
    "# Connect the data to the model and instantiate\n",
    "model = models.EncodeProcessDecode(edge_output_size=2, node_output_size=2)\n",
    "# A list of outputs, one per processing step\n",
    "output_ops_tr = model(input_ph, num_processing_steps_tr)\n",
    "output_ops_ge = model(input_ph, num_processing_steps_ge)\n",
    "\n",
    "# Training loss\n",
    "loss_ops_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "# Loss across processing steps\n",
    "loss_op_tr = sum(loss_ops_tr) / num_processing_steps_tr\n",
    "# Test/generalization loss\n",
    "loss_ops_ge = create_loss_ops(target_ph, output_ops_ge)\n",
    "loss_op_ge = loss_ops_ge[-1]  # Loss from final processing step\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 1.3e-3\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "step_op = optimizer.minimize(loss_op_tr)\n",
    "\n",
    "# Lets an iterable of TF graphs be output from a session as NP graphs\n",
    "input_ph, target_ph = make_all_runnable_in_session(input_ph, target_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# RESET SESSION\n",
    "#########\n",
    "try:\n",
    "    sess.close()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "last_iteration = 0\n",
    "logged_iterations = []\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []\n",
    "losses_ge = []\n",
    "corrects_ge = []\n",
    "solveds_ge = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# (iteration number), T (elapsed seconds), Ltr (training loss), Lge (test/generalization loss), Ctr (training fraction nodes/edges labeled correctly), Str (training fraction examples solved correctly), Cge (test/generalization fraction nodes/edges labeled correctly), Sge (test/generalization fraction examples solved correctly)\n",
      "# 00055, T 21.1, Ltr 0.6438, Lge 0.6453, Ctr 0.6318, Str 0.0000, Cge 0.6264, Sge 0.0000\n",
      "# 00137, T 40.4, Ltr 0.4320, Lge 0.4373, Ctr 0.8099, Str 0.0000, Cge 0.8183, Sge 0.0312\n",
      "# 00223, T 60.5, Ltr 0.4202, Lge 0.4645, Ctr 0.8226, Str 0.0100, Cge 0.7927, Sge 0.0312\n",
      "# 00308, T 80.6, Ltr 0.4477, Lge 0.4377, Ctr 0.8021, Str 0.0000, Cge 0.8017, Sge 0.0000\n",
      "# 00394, T 100.7, Ltr 0.4174, Lge 0.3874, Ctr 0.8186, Str 0.0300, Cge 0.8356, Sge 0.0312\n",
      "# 00480, T 120.9, Ltr 0.4220, Lge 0.3999, Ctr 0.8113, Str 0.0100, Cge 0.8177, Sge 0.0000\n",
      "# 00565, T 141.0, Ltr 0.4138, Lge 0.4103, Ctr 0.8166, Str 0.0100, Cge 0.8124, Sge 0.0312\n",
      "# 00651, T 161.0, Ltr 0.4209, Lge 0.4045, Ctr 0.8188, Str 0.0200, Cge 0.8253, Sge 0.0000\n",
      "# 00736, T 181.0, Ltr 0.4037, Lge 0.4035, Ctr 0.8298, Str 0.0000, Cge 0.8201, Sge 0.0000\n",
      "# 00822, T 201.1, Ltr 0.4199, Lge 0.4069, Ctr 0.8212, Str 0.0200, Cge 0.8263, Sge 0.0000\n",
      "# 00908, T 221.2, Ltr 0.3918, Lge 0.3801, Ctr 0.8279, Str 0.0200, Cge 0.8305, Sge 0.0000\n",
      "# 00994, T 241.3, Ltr 0.4062, Lge 0.3744, Ctr 0.8262, Str 0.0300, Cge 0.8527, Sge 0.0312\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# TRAINING\n",
    "#########\n",
    "# How much time between logging and printing the current results.\n",
    "log_every_seconds = 20\n",
    "\n",
    "print(\"# (iteration number), T (elapsed seconds), \"\n",
    "      \"Ltr (training loss), Lge (test/generalization loss), \"\n",
    "      \"Ctr (training fraction nodes/edges labeled correctly), \"\n",
    "      \"Str (training fraction examples solved correctly), \"\n",
    "      \"Cge (test/generalization fraction nodes/edges labeled correctly), \"\n",
    "      \"Sge (test/generalization fraction examples solved correctly)\")\n",
    "\n",
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "for iteration in range(last_iteration, num_training_iterations):\n",
    "    last_iteration = iteration\n",
    "\n",
    "    feed_dict, _ = create_feed_dict(batch_size_tr, input_ph, target_ph)\n",
    "    train_values = sess.run({\n",
    "        \"step\": step_op,\n",
    "        \"target\": target_ph,\n",
    "        \"loss\": loss_op_tr,\n",
    "        \"outputs\": output_ops_tr\n",
    "    }, feed_dict=feed_dict)\n",
    "\n",
    "    the_time = time.time()\n",
    "    elapsed_since_last_log = the_time - last_log_time\n",
    "\n",
    "    if elapsed_since_last_log > log_every_seconds:\n",
    "        last_log_time = the_time\n",
    "\n",
    "        feed_dict, raw_graphs = create_feed_dict(\n",
    "            batch_size_ge, input_ph, target_ph)\n",
    "        test_values = sess.run({\n",
    "            \"target\": target_ph,\n",
    "            \"loss\": loss_op_ge,\n",
    "            \"outputs\": output_ops_ge\n",
    "        }, feed_dict=feed_dict)\n",
    "\n",
    "        correct_tr, solved_tr = compute_accuracy(\n",
    "            train_values[\"target\"], train_values[\"outputs\"][-1], use_edges=True)\n",
    "        correct_ge, solved_ge = compute_accuracy(\n",
    "            test_values[\"target\"], test_values[\"outputs\"][-1], use_edges=True)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        losses_tr.append(train_values[\"loss\"])\n",
    "        corrects_tr.append(correct_tr)\n",
    "        solveds_tr.append(solved_tr)\n",
    "        losses_ge.append(test_values[\"loss\"])\n",
    "        corrects_ge.append(correct_ge)\n",
    "        solveds_ge.append(solved_ge)\n",
    "        logged_iterations.append(iteration)\n",
    "\n",
    "        print(\"# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Ctr {:.4f}, Str\"\n",
    "              \" {:.4f}, Cge {:.4f}, Sge {:.4f}\".format(\n",
    "                  iteration, elapsed, train_values[\"loss\"], test_values[\"loss\"],\n",
    "                  correct_tr, solved_tr, correct_ge, solved_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# POST-TRAINING TESTING\n",
    "#########\n",
    "test_batch_size = 1000\n",
    "num_processing_steps_test = 10\n",
    "\n",
    "test_input_ph, test_target_ph = create_placeholders(test_batch_size)\n",
    "test_output_ops = model(test_input_ph, num_processing_steps_test)\n",
    "\n",
    "test_loss_ops = create_loss_ops(test_target_ph, test_output_ops)\n",
    "test_loss_op = test_loss_ops[-1]\n",
    "\n",
    "test_input_ph, test_target_ph = make_all_runnable_in_session(\n",
    "    test_input_ph, test_target_ph)\n",
    "\n",
    "test_feed_dict, test_input_graphs = create_feed_dict(\n",
    "    test_batch_size, test_input_ph, test_target_ph)\n",
    "test_values = sess.run({\n",
    "    \"target\": test_target_ph,\n",
    "    \"loss\": test_loss_op,\n",
    "    \"outputs\": test_output_ops\n",
    "}, feed_dict=test_feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# MODEL & RESULTS STORAGE\n",
    "#########\n",
    "\n",
    "\n",
    "# Store the model\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, \"data/pickles/model.ckpt\")\n",
    "\n",
    "# Store the training statistics\n",
    "train_stats = pd.DataFrame(np.array([logged_iterations, losses_tr, losses_ge,\n",
    "                                     corrects_tr, solveds_tr, corrects_ge, solveds_ge]).T,\n",
    "                           columns=[\"iteration\", \"loss_tr\", \"loss_ge\", \"correct_tr\", \"solved_tr\",\n",
    "                                    \"correct_ge\", \"solved_ge\"])\n",
    "train_stats.to_pickle(\"data/pickles/train_stats.pkl\")\n",
    "\n",
    "# Store the test results\n",
    "pickle.dump({\n",
    "    \"outputs\": utils_np.graphs_tuple_to_networkxs(test_values[\"outputs\"][-1]),\n",
    "    \"targets\": utils_np.graphs_tuple_to_networkxs(test_values[\"target\"]),\n",
    "    \"inputs\": test_input_graphs,\n",
    "}, open(\"data/pickles/test_results.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
