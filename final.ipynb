{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# IMPORTS\n",
    "#########\n",
    "import random as rand\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "from tsp_solver.greedy import solve_tsp as solve\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets import utils_np\n",
    "from graph_nets.demos import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# UTILS\n",
    "#########\n",
    "\n",
    "# This function generates a Hamiltonian graph by generating a complete graph\n",
    "# with random initial weights.\n",
    "def create_random_graph(node_range=(5, 9), prob=0.25, weight_range=(1, 10)):\n",
    "    n_nodes = rand.randint(*node_range)\n",
    "\n",
    "    G = nx.complete_graph(n_nodes)\n",
    "    H = G.copy()\n",
    "    for u, v, w in G.edges(data=True):\n",
    "        H[u][v][\"weight\"] = rand.randint(*weight_range)\n",
    "\n",
    "    return H\n",
    "\n",
    "# this function solves the TSP problem in a graph with\n",
    "# a TSP solver from third-party library\n",
    "def solve_tsp(graph):\n",
    "    adj_matrix = nx.adjacency_matrix(graph)\n",
    "    hamil_path = solve(adj_matrix.todense().tolist())\n",
    "\n",
    "    path_edges = [(hamil_path[i], hamil_path[i + 1])\n",
    "                  for i in range(len(hamil_path) - 1)]\n",
    "    path_edges.append((hamil_path[-1], hamil_path[0]))\n",
    "\n",
    "    for u, v in graph.edges():\n",
    "        graph[u][v][\"solution\"] = int(\n",
    "            any({u, v}.issubset({src, targ}) for src, targ in path_edges))\n",
    "\n",
    "    solution_dict = {v: False for v in graph.nodes()}\n",
    "    for u, v in path_edges:\n",
    "        solution_dict[u] = True\n",
    "        solution_dict[v] = True\n",
    "\n",
    "    nx.set_node_attributes(graph, solution_dict, \"solution\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "# pad a one hot vector\n",
    "def to_one_hot(indices, max_value, axis=-1):\n",
    "    one_hot = np.eye(max_value)[indices]\n",
    "    if axis not in (-1, one_hot.ndim):\n",
    "        one_hot = np.moveaxis(one_hot, -1, axis)\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "# from a Hamiltonian graph that contains all information,\n",
    "# this function generates an input graph and a target graph\n",
    "# with the necessary information encoded in each graph.\n",
    "def graph_to_input_target(graph):\n",
    "    def create_feature(attr, fields):\n",
    "        return np.hstack([np.array(attr[field], dtype=float) for field in fields])\n",
    "\n",
    "    # feature fields\n",
    "    input_node_fields = (\"solution\",)\n",
    "    input_edge_fields = (\"weight\",)\n",
    "    target_node_fields = (\"solution\",)\n",
    "    target_edge_fields = (\"solution\",)\n",
    "\n",
    "    input_graph = graph.copy()\n",
    "    target_graph = graph.copy()\n",
    "\n",
    "    # Generate Node Features\n",
    "    solution_length = 0\n",
    "    for node_index, node_feature in graph.nodes(data=True):\n",
    "        input_graph.add_node(\n",
    "            node_index, features=create_feature(node_feature, input_node_fields))\n",
    "        target_node = to_one_hot(\n",
    "            create_feature(node_feature, target_node_fields).astype(int), 2)[0]\n",
    "        target_graph.add_node(node_index, features=target_node)\n",
    "\n",
    "    # Generate Edge Features\n",
    "    for receiver, sender, features in graph.edges(data=True):\n",
    "        input_graph.add_edge(\n",
    "            sender, receiver, features=create_feature(features, input_edge_fields))\n",
    "        target_edge = to_one_hot(\n",
    "            create_feature(features, target_edge_fields).astype(int), 2)[0]\n",
    "        target_graph.add_edge(sender, receiver, features=target_edge)\n",
    "        solution_length += features[\"weight\"] * features[\"solution\"]\n",
    "\n",
    "    input_graph.graph[\"features\"] = np.array([0.0])\n",
    "    target_graph.graph[\"features\"] = np.array([solution_length], dtype=float)\n",
    "\n",
    "    return input_graph, target_graph\n",
    "\n",
    "# generate_nx_graphs wraps up the previous functions\n",
    "# and serves as a helper function to conveniently\n",
    "# generates graphs for training/testing\n",
    "def generate_networkx_graphs(num_graphs, node_range=(5, 9), prob=0.25, weight_range=(1, 10)):\n",
    "    input_graphs = []\n",
    "    target_graphs = []\n",
    "    graphs = []\n",
    "\n",
    "    for i in range(num_graphs):\n",
    "        graph = create_random_graph(node_range, prob, weight_range)\n",
    "        graph = solve_tsp(graph)\n",
    "        input_graph, target_graph = graph_to_input_target(graph)\n",
    "        input_graphs.append(input_graph)\n",
    "        target_graphs.append(target_graph)\n",
    "        graphs.append(graph)\n",
    "\n",
    "    return input_graphs, target_graphs, graphs\n",
    "\n",
    "\n",
    "#########\n",
    "# EXPORTS\n",
    "#########\n",
    "\n",
    "# Create a placeholder for feeding graphs to the NN\n",
    "# uses API provided by deepming GNN library to do that\n",
    "def create_placeholders(num_graphs):\n",
    "    input_graphs, target_graphs, _ = generate_networkx_graphs(num_graphs)\n",
    "    input_ph = utils_tf.placeholders_from_networkxs(input_graphs)\n",
    "    target_ph = utils_tf.placeholders_from_networkxs(target_graphs)\n",
    "    return input_ph, target_ph\n",
    "\n",
    "# Make all arguments runable\n",
    "def make_all_runnable_in_session(*args):\n",
    "    \"\"\"Lets an iterable of TF graphs be output from a session as NP graphs.\"\"\"\n",
    "    return [utils_tf.make_runnable_in_session(a) for a in args]\n",
    "\n",
    "# Create tensorflow feed dict by padding info\n",
    "def create_feed_dict(num_graphs, input_ph, target_ph):\n",
    "    \"\"\"Creates placeholders for the model training and evaluation.\"\"\"\n",
    "\n",
    "    inputs, targets, raw_graphs = generate_networkx_graphs(num_graphs)\n",
    "    input_graphs = utils_np.networkxs_to_graphs_tuple(inputs)\n",
    "    target_graphs = utils_np.networkxs_to_graphs_tuple(targets)\n",
    "    feed_dict = {input_ph: input_graphs, target_ph: target_graphs}\n",
    "\n",
    "    return feed_dict, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# TRAINING_HELPER (THAT CAN BE TUNED)\n",
    "#########\n",
    "\n",
    "# use edge information for computing losses\n",
    "def create_loss_ops(target_op, output_ops):\n",
    "    return [\n",
    "        tf.losses.softmax_cross_entropy(target_op.edges, output_op.edges)\n",
    "        for output_op in output_ops\n",
    "    ]\n",
    "\n",
    "# Compute classification accuracy\n",
    "def compute_accuracy(target, output, use_nodes=True, use_edges=False):\n",
    "    if not use_nodes and not use_edges:\n",
    "        raise ValueError(\"Nodes or edges (or both) must be used\")\n",
    "\n",
    "    tdds = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "    odds = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "\n",
    "    cs, ss = [], []\n",
    "    for td, od in zip(tdds, odds):\n",
    "\n",
    "        xe = np.argmax(td[\"edges\"], axis=-1)\n",
    "        ye = np.argmax(od[\"edges\"], axis=-1)\n",
    "\n",
    "        c = [xe == ye] if use_edges else []\n",
    "        c = np.concatenate(c, axis=0)\n",
    "\n",
    "        s = np.all(c)\n",
    "        cs.append(c)\n",
    "        ss.append(s)\n",
    "\n",
    "    correct = np.mean(np.concatenate(cs, axis=0))\n",
    "    solved = np.mean(np.stack(ss))\n",
    "\n",
    "    return correct, solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/roger/Library/Python/3.7/lib/python/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/roger/Library/Python/3.7/lib/python/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/roger/Library/Python/3.7/lib/python/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roger/Library/Python/3.7/lib/python/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# MODEL AND HYPERPARAMETERS SETUP\n",
    "#########\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 2\n",
    "rand = np.random.RandomState(seed=seed)\n",
    "\n",
    "# Model parameters; no. of message-passing steps\n",
    "num_processing_steps_tr = 10\n",
    "num_processing_steps_ge = 10\n",
    "\n",
    "# Data / training parameters\n",
    "num_training_iterations = 2000\n",
    "batch_size_tr = 200\n",
    "batch_size_ge = 50\n",
    "\n",
    "# Input and target placeholders\n",
    "input_ph, target_ph = create_placeholders(batch_size_tr)\n",
    "\n",
    "# Connect the data to the model and instantiate\n",
    "model = models.EncodeProcessDecode(edge_output_size=2, node_output_size=2)\n",
    "# A list of outputs, one per processing step\n",
    "output_ops_tr = model(input_ph, num_processing_steps_tr)\n",
    "output_ops_ge = model(input_ph, num_processing_steps_ge)\n",
    "\n",
    "# Training loss\n",
    "loss_ops_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "# Loss across processing steps\n",
    "loss_op_tr = sum(loss_ops_tr) / num_processing_steps_tr\n",
    "# Test/generalization loss\n",
    "loss_ops_ge = create_loss_ops(target_ph, output_ops_ge)\n",
    "loss_op_ge = loss_ops_ge[-1]  # Loss from final processing step\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 1.3e-3\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "step_op = optimizer.minimize(loss_op_tr)\n",
    "\n",
    "# Lets an iterable of TF graphs be output from a session as NP graphs\n",
    "input_ph, target_ph = make_all_runnable_in_session(input_ph, target_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# RESET SESSION\n",
    "#########\n",
    "try:\n",
    "    sess.close()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "last_iteration = 0\n",
    "logged_iterations = []\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []\n",
    "losses_ge = []\n",
    "corrects_ge = []\n",
    "solveds_ge = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/roger/Library/Python/3.7/lib/python/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from data/pickles/model.ckpt\n",
      "USING MODEL FOUND ON LOCAL STORAGE\n"
     ]
    }
   ],
   "source": [
    "restored_model_flag = False\n",
    "try:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"data/pickles/model.ckpt\")\n",
    "    restored_model_flag = True\n",
    "    print(\"USING MODEL FOUND ON LOCAL STORAGE\")\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# (iteration number), T (elapsed seconds), Ltr (training loss), Lge (test/generalization loss), Ctr (training fraction nodes/edges labeled correctly), Str (training fraction examples solved correctly), Cge (test/generalization fraction nodes/edges labeled correctly), Sge (test/generalization fraction examples solved correctly)\n",
      "# 00030, T 21.3, Ltr 0.3545, Lge 0.2811, Ctr 0.8713, Str 0.1350, Cge 0.8835, Sge 0.2400\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# TRAINING\n",
    "#########\n",
    "# How much time between logging and printing the current results.\n",
    "log_every_seconds = 20\n",
    "\n",
    "print(\"# (iteration number), T (elapsed seconds), \"\n",
    "      \"Ltr (training loss), Lge (test/generalization loss), \"\n",
    "      \"Ctr (training fraction nodes/edges labeled correctly), \"\n",
    "      \"Str (training fraction examples solved correctly), \"\n",
    "      \"Cge (test/generalization fraction nodes/edges labeled correctly), \"\n",
    "      \"Sge (test/generalization fraction examples solved correctly)\")\n",
    "\n",
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "for iteration in range(last_iteration, num_training_iterations):\n",
    "    last_iteration = iteration\n",
    "\n",
    "    feed_dict, _ = create_feed_dict(batch_size_tr, input_ph, target_ph)\n",
    "    train_values = sess.run({\n",
    "        \"step\": step_op,\n",
    "        \"target\": target_ph,\n",
    "        \"loss\": loss_op_tr,\n",
    "        \"outputs\": output_ops_tr\n",
    "    }, feed_dict=feed_dict)\n",
    "\n",
    "    the_time = time.time()\n",
    "    elapsed_since_last_log = the_time - last_log_time\n",
    "\n",
    "    if elapsed_since_last_log > log_every_seconds:\n",
    "        last_log_time = the_time\n",
    "\n",
    "        feed_dict, raw_graphs = create_feed_dict(\n",
    "            batch_size_ge, input_ph, target_ph)\n",
    "        test_values = sess.run({\n",
    "            \"target\": target_ph,\n",
    "            \"loss\": loss_op_ge,\n",
    "            \"outputs\": output_ops_ge\n",
    "        }, feed_dict=feed_dict)\n",
    "\n",
    "        correct_tr, solved_tr = compute_accuracy(\n",
    "            train_values[\"target\"], train_values[\"outputs\"][-1], use_edges=True)\n",
    "        correct_ge, solved_ge = compute_accuracy(\n",
    "            test_values[\"target\"], test_values[\"outputs\"][-1], use_edges=True)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        losses_tr.append(train_values[\"loss\"])\n",
    "        corrects_tr.append(correct_tr)\n",
    "        solveds_tr.append(solved_tr)\n",
    "        losses_ge.append(test_values[\"loss\"])\n",
    "        corrects_ge.append(correct_ge)\n",
    "        solveds_ge.append(solved_ge)\n",
    "        logged_iterations.append(iteration)\n",
    "\n",
    "        print(\"# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Ctr {:.4f}, Str\"\n",
    "              \" {:.4f}, Cge {:.4f}, Sge {:.4f}\".format(\n",
    "                  iteration, elapsed, train_values[\"loss\"], test_values[\"loss\"],\n",
    "                  correct_tr, solved_tr, correct_ge, solved_ge))\n",
    "        if restored_model_flag:\n",
    "            print(\"Breaking...\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# POST-TRAINING TESTING\n",
    "#########\n",
    "test_batch_size = 1000\n",
    "num_processing_steps_test = 10\n",
    "\n",
    "test_input_ph, test_target_ph = create_placeholders(test_batch_size)\n",
    "test_output_ops = model(test_input_ph, num_processing_steps_test)\n",
    "\n",
    "test_loss_ops = create_loss_ops(test_target_ph, test_output_ops)\n",
    "test_loss_op = test_loss_ops[-1]\n",
    "\n",
    "test_input_ph, test_target_ph = make_all_runnable_in_session(\n",
    "    test_input_ph, test_target_ph)\n",
    "\n",
    "test_feed_dict, test_input_graphs = create_feed_dict(\n",
    "    test_batch_size, test_input_ph, test_target_ph)\n",
    "test_values = sess.run({\n",
    "    \"target\": test_target_ph,\n",
    "    \"loss\": test_loss_op,\n",
    "    \"outputs\": test_output_ops\n",
    "}, feed_dict=test_feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# MODEL & RESULTS STORAGE\n",
    "#########\n",
    "\n",
    "\n",
    "# Store the model\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, \"data/pickles/model.ckpt\")\n",
    "\n",
    "# Store the training statistics\n",
    "train_stats = pd.DataFrame(np.array([logged_iterations, losses_tr, losses_ge,\n",
    "                                     corrects_tr, solveds_tr, corrects_ge, solveds_ge]).T,\n",
    "                           columns=[\"iteration\", \"loss_tr\", \"loss_ge\", \"correct_tr\", \"solved_tr\",\n",
    "                                    \"correct_ge\", \"solved_ge\"])\n",
    "train_stats.to_pickle(\"data/pickles/train_stats.pkl\")\n",
    "\n",
    "output_graphs = utils_np.graphs_tuple_to_networkxs(test_values[\"outputs\"][-1])\n",
    "target_graphs = utils_np.graphs_tuple_to_networkxs(test_values[\"target\"])\n",
    "input_graphs = test_input_graphs\n",
    "\n",
    "# Store the test results\n",
    "pickle.dump({\n",
    "    \"outputs\": output_graphs,\n",
    "    \"targets\": target_graphs,\n",
    "    \"inputs\": input_graphs,\n",
    "}, open(\"data/pickles/test_results.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# EVAL HELPERS\n",
    "#########\n",
    "def is_solution_edge(feature_vector):\n",
    "    # if it's solution, one hot vector is (0, 1)\n",
    "    if feature_vector[1] > feature_vector[0]: return True\n",
    "    return False\n",
    "\n",
    "# return a dictionary with vertices as keys\n",
    "# each item contains its neighbor vertices connected\n",
    "# by the labelled edges\n",
    "def get_neighbor_dict(my_graph):\n",
    "    ret_dict = {}\n",
    "    edge_list = [i for i in my_graph.edges(data = True)]\n",
    "    output_label_cnt = 0\n",
    "    for i in edge_list:\n",
    "        if is_solution_edge(i[2][\"features\"]):\n",
    "            try:\n",
    "                ret_dict[i[0]].append(i[1])\n",
    "            except KeyError:\n",
    "                ret_dict[i[0]] = [i[1]]\n",
    "            try:\n",
    "                ret_dict[i[1]].append(i[0])\n",
    "            except KeyError:\n",
    "                ret_dict[i[1]] = [i[0]]\n",
    "            output_label_cnt += 1\n",
    "    return ret_dict, output_label_cnt\n",
    "\n",
    "def hamiltonian_path_check(my_graph):\n",
    "    neighbor_dict, my_count = get_neighbor_dict(my_graph)\n",
    "    my_vertices = [i for i in my_graph.nodes()]\n",
    "    visited_dict = [False] * len(my_vertices)\n",
    "    # since hamiltonian path is closed; choose a random vertex to start\n",
    "    # actually nvm, maybe just use the first vertex...\n",
    "    start_vertex = my_vertices[0]\n",
    "    # test 1: every vertex is in the neighbor_dict\n",
    "    for v in my_vertices:\n",
    "        try:\n",
    "            neighbor_dict[v]\n",
    "        except KeyError:\n",
    "            return False, neighbor_dict, my_count\n",
    "    # test 2: every vertex connects to two and only two other vertices\n",
    "    for k, i in neighbor_dict.items():\n",
    "        if len(i) != 2: return False, neighbor_dict, my_count\n",
    "    # test 3: start from a node and perform the walk, we will end up at the same node\n",
    "    current_vertex = start_vertex\n",
    "    next_vertex = neighbor_dict[start_vertex][0]\n",
    "    while True:\n",
    "        visited_dict[current_vertex] = True\n",
    "        if next_vertex == start_vertex: break\n",
    "        if neighbor_dict[next_vertex][0] == current_vertex:\n",
    "            current_vertex = next_vertex\n",
    "            next_vertex = neighbor_dict[next_vertex][1]\n",
    "        else:\n",
    "            current_vertex = next_vertex\n",
    "            next_vertex = neighbor_dict[next_vertex][0]\n",
    "    for mark in visited_dict:\n",
    "        if mark == False: return False, neighbor_dict, my_count\n",
    "    return True, neighbor_dict, my_count\n",
    "\n",
    "def get_ori_weight(input_graph, u, v):\n",
    "    return input_graph.get_edge_data(u, v)[\"weight\"]\n",
    "\n",
    "def evaluate(input_graphs, output_graphs, target_graphs):\n",
    "    assert len(output_graphs) == len(target_graphs)\n",
    "    assert len(input_graphs) == len(output_graphs)\n",
    "    assert isinstance(output_graphs, list)\n",
    "    assert isinstance(target_graphs, list)\n",
    "\n",
    "    n_num = len(output_graphs)\n",
    "\n",
    "    # sort graph by correctly labelled edges\n",
    "    # seperate output into bad/average/good sets\n",
    "\n",
    "    is_hamiltonian_path_count = 0\n",
    "    recall_correct_count = 0\n",
    "    recall_total_count = 0\n",
    "    precision_total_count = 0\n",
    "\n",
    "    output_solution_len_list = []\n",
    "    target_solution_len_list = []\n",
    "\n",
    "    non_output_soln_len_list = []\n",
    "    non_target_soln_len_list = []\n",
    "\n",
    "    for i in range(len(output_graphs)):\n",
    "        ret, my_dict, my_count = hamiltonian_path_check(output_graphs[i])\n",
    "        precision_total_count += my_count\n",
    "        if ret:\n",
    "            is_hamiltonian_path_count += 1\n",
    "        # get output solution length\n",
    "        output_soln_len = 0\n",
    "        for u in my_dict.keys():\n",
    "            for v in my_dict[u]:\n",
    "                if u > v: continue\n",
    "                output_soln_len += get_ori_weight(input_graphs[i], u, v)\n",
    "        # get target solution length\n",
    "        tar_soln_len = 0\n",
    "        for e in input_graphs[i].edges(data = True):\n",
    "            if e[2][\"solution\"] == 1:\n",
    "                tar_soln_len += e[2][\"weight\"]\n",
    "                recall_total_count += 1\n",
    "                try:\n",
    "                    for neighboring_vertex in my_dict[e[0]]:\n",
    "                        if neighboring_vertex == e[1]:\n",
    "                            recall_correct_count += 1\n",
    "                            break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            output_solution_len_list.append(output_soln_len)\n",
    "            target_solution_len_list.append(tar_soln_len)\n",
    "            non_output_soln_len_list.append(output_soln_len)\n",
    "            non_target_soln_len_list.append(tar_soln_len)\n",
    "    # calculate difference between output and target graphs\n",
    "    print(\"{0} out of {1} solutions are Hamiltonian.\".format(is_hamiltonian_path_count, n_num))\n",
    "    output_solution_len_list = np.array(output_solution_len_list)\n",
    "    target_solution_len_list = np.array(target_solution_len_list)\n",
    "    diff_ = output_solution_len_list - target_solution_len_list\n",
    "    print(\"Among Hamiltonian cycles found by the GNN, the average difference in solution graph with ground truth is {0:.4f}\".format(np.mean(diff_)))\n",
    "    output_vec = np.array(non_output_soln_len_list)\n",
    "    target_vec = np.array(non_target_soln_len_list)\n",
    "    diff_ = output_vec - target_vec\n",
    "    print(\"Among those that were incorrectly labelled, the average error in solution length is {0:.4f}\".format(np.mean(diff_)))\n",
    "    total_weight = 0\n",
    "    for g in input_graphs:\n",
    "        for e in g.edges(data = True):\n",
    "            total_weight += e[2][\"weight\"]\n",
    "    average_weight = total_weight / n_num\n",
    "    print(\"On average, the total weight of each graph we used is {0:.4f}. So the average relative error is {1:.4f}\".format(average_weight, np.mean(diff_) / average_weight))\n",
    "    print(\"Recall: {0:.4f}\".format(recall_correct_count / recall_total_count))\n",
    "    print(\"Precision: {0:.4f}\".format(recall_correct_count / precision_total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 out of 1000 solutions are Hamiltonian.\n",
      "Among Hamiltonian cycles found by the GNN, the average difference in solution graph with ground truth is 6.0105\n",
      "Among those that were incorrectly labelled, the average error in solution length is 6.0105\n",
      "On average, the total weight of each graph we used is 92.5670. So the average relative error is 0.0649\n",
      "Recall: 0.7891\n",
      "Precision: 0.8387\n"
     ]
    }
   ],
   "source": [
    "evaluate(input_graphs, output_graphs, target_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
