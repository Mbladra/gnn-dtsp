{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pjdoh\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from src.utils import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from graph_nets.demos import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss_ops(target_op, output_ops):\n",
    "    loss_ops = [\n",
    "        #tf.losses.softmax_cross_entropy(target_op.nodes, output_op.nodes) +\n",
    "        tf.losses.softmax_cross_entropy(target_op.edges, output_op.edges)\n",
    "        for output_op in output_ops\n",
    "    ]\n",
    "    return loss_ops\n",
    "\n",
    "def compute_accuracy(target, output, use_nodes=True, use_edges=False):\n",
    "    if not use_nodes and not use_edges:\n",
    "        raise ValueError(\"Nodes or edges (or both) must be used\")\n",
    "    tdds = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "    odds = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "    cs = []\n",
    "    ss = []\n",
    "    for td, od in zip(tdds, odds):\n",
    "        xn = np.argmax(td[\"nodes\"], axis=-1)\n",
    "        yn = np.argmax(od[\"nodes\"], axis=-1)\n",
    "        xe = np.argmax(td[\"edges\"], axis=-1)\n",
    "        ye = np.argmax(od[\"edges\"], axis=-1)\n",
    "        c = []\n",
    "        if use_nodes:\n",
    "            c.append(xn == yn)\n",
    "        if use_edges:\n",
    "            c.append(xe == ye)\n",
    "        c = np.concatenate(c, axis=0)\n",
    "        s = np.all(c)\n",
    "        cs.append(c)\n",
    "        ss.append(s)\n",
    "    correct = np.mean(np.concatenate(cs, axis=0))\n",
    "    solved = np.mean(np.stack(ss))\n",
    "    return correct, solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pjdoh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Model Setup\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 2\n",
    "rand = np.random.RandomState(seed=seed)\n",
    "\n",
    "# Model parameters.\n",
    "# Number of processing (message-passing) steps.\n",
    "num_processing_steps_tr = 10\n",
    "num_processing_steps_ge = 10\n",
    "\n",
    "# Data / training parameters.\n",
    "num_training_iterations = 10000\n",
    "theta = 20  # Large values (1000+) make trees. Try 20-60 for good non-trees.\n",
    "batch_size_tr = 32\n",
    "batch_size_ge = 100\n",
    "# Number of nodes per graph sampled uniformly from this range.\n",
    "num_nodes_min_max_tr = (8, 17)\n",
    "num_nodes_min_max_ge = (16, 33)\n",
    "\n",
    "# Data.\n",
    "# Input and target placeholders.\n",
    "input_ph, target_ph = create_placeholders(batch_size_tr)\n",
    "\n",
    "# Connect the data to the model.\n",
    "# Instantiate the model.\n",
    "model = models.EncodeProcessDecode(edge_output_size=2, node_output_size=2)\n",
    "# A list of outputs, one per processing step.\n",
    "output_ops_tr = model(input_ph, num_processing_steps_tr)\n",
    "output_ops_ge = model(input_ph, num_processing_steps_ge)\n",
    "\n",
    "# Training loss.\n",
    "loss_ops_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "# Loss across processing steps.e\n",
    "loss_op_tr = sum(loss_ops_tr) / num_processing_steps_tr\n",
    "# Test/generalization loss.\n",
    "loss_ops_ge = create_loss_ops(target_ph, output_ops_ge)\n",
    "loss_op_ge = loss_ops_ge[-1]  # Loss from final processing step.\n",
    "\n",
    "# Optimizer.\n",
    "learning_rate = 1e-3\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "step_op = optimizer.minimize(loss_op_tr)\n",
    "\n",
    "# Lets an iterable of TF graphs be output from a session as NP graphs.\n",
    "input_ph, target_ph = make_all_runnable_in_session(input_ph, target_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Session\n",
    "#@title Reset session  { form-width: \"30%\" }\n",
    "\n",
    "# This cell resets the Tensorflow session, but keeps the same computational\n",
    "# graph.\n",
    "\n",
    "try:\n",
    "    sess.close()\n",
    "except NameError:\n",
    "    pass\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "last_iteration = 0\n",
    "logged_iterations = []\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []\n",
    "losses_ge = []\n",
    "corrects_ge = []\n",
    "solveds_ge = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# (iteration number), T (elapsed seconds), Ltr (training loss), Lge (test/generalization loss), Ctr (training fraction nodes/edges labeled correctly), Str (training fraction examples solved correctly), Cge (test/generalization fraction nodes/edges labeled correctly), Sge (test/generalization fraction examples solved correctly)\n",
      "# 00132, T 21.1, Ltr 0.4623, Lge 0.4599, Ctr 0.8068, Str 0.0000, Cge 0.8032, Sge 0.0000\n",
      "# 00326, T 40.3, Ltr 0.4041, Lge 0.4066, Ctr 0.8342, Str 0.0000, Cge 0.8258, Sge 0.0000\n",
      "# 00532, T 60.3, Ltr 0.4152, Lge 0.3842, Ctr 0.8250, Str 0.0000, Cge 0.8351, Sge 0.0100\n",
      "# 00727, T 80.3, Ltr 0.3947, Lge 0.3832, Ctr 0.8287, Str 0.0312, Cge 0.8370, Sge 0.0200\n",
      "# 00921, T 100.4, Ltr 0.3808, Lge 0.3670, Ctr 0.8297, Str 0.0000, Cge 0.8417, Sge 0.0200\n",
      "# 01104, T 120.5, Ltr 0.4145, Lge 0.3795, Ctr 0.8176, Str 0.0312, Cge 0.8333, Sge 0.0100\n",
      "# 01301, T 140.5, Ltr 0.3800, Lge 0.3838, Ctr 0.8517, Str 0.0000, Cge 0.8377, Sge 0.0000\n",
      "# 01485, T 160.5, Ltr 0.3952, Lge 0.3699, Ctr 0.8329, Str 0.0000, Cge 0.8359, Sge 0.0300\n",
      "# 01687, T 180.7, Ltr 0.3672, Lge 0.3443, Ctr 0.8493, Str 0.0000, Cge 0.8540, Sge 0.0300\n",
      "# 01885, T 200.7, Ltr 0.3765, Lge 0.3735, Ctr 0.8392, Str 0.0000, Cge 0.8415, Sge 0.0000\n",
      "# 02081, T 220.8, Ltr 0.3906, Lge 0.3654, Ctr 0.8361, Str 0.0000, Cge 0.8420, Sge 0.0100\n",
      "# 02265, T 240.8, Ltr 0.3886, Lge 0.3832, Ctr 0.8353, Str 0.0312, Cge 0.8398, Sge 0.0100\n",
      "# 02472, T 260.9, Ltr 0.3984, Lge 0.4013, Ctr 0.8417, Str 0.1250, Cge 0.8239, Sge 0.0100\n",
      "# 02666, T 280.9, Ltr 0.3807, Lge 0.3589, Ctr 0.8500, Str 0.0312, Cge 0.8511, Sge 0.0100\n",
      "# 02871, T 300.9, Ltr 0.3976, Lge 0.3758, Ctr 0.8213, Str 0.0000, Cge 0.8406, Sge 0.0200\n",
      "# 03071, T 321.2, Ltr 0.3751, Lge 0.3585, Ctr 0.8422, Str 0.0000, Cge 0.8518, Sge 0.0100\n",
      "# 03274, T 341.3, Ltr 0.3516, Lge 0.3260, Ctr 0.8597, Str 0.0000, Cge 0.8604, Sge 0.0500\n",
      "# 03458, T 361.3, Ltr 0.3595, Lge 0.3479, Ctr 0.8449, Str 0.0000, Cge 0.8500, Sge 0.0200\n",
      "# 03620, T 381.4, Ltr 0.3571, Lge 0.3371, Ctr 0.8629, Str 0.0000, Cge 0.8538, Sge 0.0400\n",
      "# 03800, T 401.4, Ltr 0.3536, Lge 0.3321, Ctr 0.8567, Str 0.0625, Cge 0.8571, Sge 0.0800\n",
      "# 04002, T 421.4, Ltr 0.3459, Lge 0.3418, Ctr 0.8627, Str 0.0000, Cge 0.8520, Sge 0.0400\n",
      "# 04211, T 441.5, Ltr 0.3333, Lge 0.3370, Ctr 0.8736, Str 0.0312, Cge 0.8609, Sge 0.0400\n",
      "# 04404, T 461.6, Ltr 0.3302, Lge 0.3179, Ctr 0.8784, Str 0.0625, Cge 0.8687, Sge 0.0600\n",
      "# 04602, T 481.6, Ltr 0.3313, Lge 0.3273, Ctr 0.8661, Str 0.0000, Cge 0.8552, Sge 0.0400\n",
      "# 04807, T 501.7, Ltr 0.3761, Lge 0.3369, Ctr 0.8365, Str 0.0000, Cge 0.8498, Sge 0.0000\n",
      "# 05010, T 521.7, Ltr 0.3419, Lge 0.3306, Ctr 0.8651, Str 0.0625, Cge 0.8584, Sge 0.0500\n",
      "# 05214, T 541.7, Ltr 0.3632, Lge 0.3135, Ctr 0.8634, Str 0.0625, Cge 0.8632, Sge 0.0400\n",
      "# 05421, T 561.8, Ltr 0.3538, Lge 0.3123, Ctr 0.8474, Str 0.0000, Cge 0.8633, Sge 0.1200\n",
      "# 05615, T 581.8, Ltr 0.3602, Lge 0.3169, Ctr 0.8591, Str 0.0938, Cge 0.8689, Sge 0.0900\n",
      "# 05815, T 601.9, Ltr 0.3780, Lge 0.3208, Ctr 0.8499, Str 0.0000, Cge 0.8684, Sge 0.0700\n",
      "# 06007, T 622.0, Ltr 0.3300, Lge 0.3161, Ctr 0.8775, Str 0.0625, Cge 0.8691, Sge 0.0900\n",
      "# 06191, T 642.2, Ltr 0.3599, Lge 0.3428, Ctr 0.8666, Str 0.0000, Cge 0.8475, Sge 0.0200\n",
      "# 06364, T 662.1, Ltr 0.3365, Lge 0.3180, Ctr 0.8652, Str 0.1250, Cge 0.8690, Sge 0.0700\n",
      "# 06563, T 682.2, Ltr 0.3384, Lge 0.2891, Ctr 0.8722, Str 0.0312, Cge 0.8805, Sge 0.1000\n",
      "# 06761, T 702.3, Ltr 0.3351, Lge 0.2969, Ctr 0.8681, Str 0.0312, Cge 0.8722, Sge 0.0400\n",
      "# 06965, T 722.3, Ltr 0.3074, Lge 0.3050, Ctr 0.8859, Str 0.1562, Cge 0.8685, Sge 0.1100\n",
      "# 07168, T 742.4, Ltr 0.3254, Lge 0.3212, Ctr 0.8764, Str 0.0625, Cge 0.8752, Sge 0.1200\n",
      "# 07374, T 762.4, Ltr 0.3528, Lge 0.3186, Ctr 0.8445, Str 0.0938, Cge 0.8598, Sge 0.0800\n",
      "# 07579, T 782.9, Ltr 0.3229, Lge 0.2907, Ctr 0.8730, Str 0.1250, Cge 0.8762, Sge 0.0800\n",
      "# 07780, T 802.9, Ltr 0.3603, Lge 0.2959, Ctr 0.8561, Str 0.0312, Cge 0.8788, Sge 0.0700\n",
      "# 07985, T 823.4, Ltr 0.3773, Lge 0.3055, Ctr 0.8484, Str 0.0312, Cge 0.8705, Sge 0.1500\n",
      "# 08188, T 843.1, Ltr 0.3346, Lge 0.3172, Ctr 0.8738, Str 0.0312, Cge 0.8667, Sge 0.0500\n",
      "# 08396, T 863.1, Ltr 0.3438, Lge 0.3169, Ctr 0.8680, Str 0.0938, Cge 0.8660, Sge 0.1000\n",
      "# 08594, T 883.1, Ltr 0.3442, Lge 0.3213, Ctr 0.8777, Str 0.0938, Cge 0.8633, Sge 0.0600\n",
      "# 08790, T 903.2, Ltr 0.3244, Lge 0.2905, Ctr 0.8729, Str 0.0000, Cge 0.8736, Sge 0.1000\n",
      "# 08965, T 923.3, Ltr 0.3633, Lge 0.3090, Ctr 0.8559, Str 0.0938, Cge 0.8720, Sge 0.1000\n",
      "# 09159, T 943.3, Ltr 0.3549, Lge 0.2874, Ctr 0.8712, Str 0.0625, Cge 0.8806, Sge 0.0800\n",
      "# 09360, T 963.3, Ltr 0.2956, Lge 0.3049, Ctr 0.8964, Str 0.1250, Cge 0.8756, Sge 0.1200\n",
      "# 09560, T 983.4, Ltr 0.3449, Lge 0.3014, Ctr 0.8697, Str 0.0000, Cge 0.8808, Sge 0.1100\n",
      "# 09760, T 1003.4, Ltr 0.3351, Lge 0.3204, Ctr 0.8690, Str 0.0938, Cge 0.8692, Sge 0.0700\n",
      "# 09958, T 1023.5, Ltr 0.3602, Lge 0.2920, Ctr 0.8479, Str 0.0625, Cge 0.8788, Sge 0.1000\n"
     ]
    }
   ],
   "source": [
    "#@title Run training  { form-width: \"30%\" }\n",
    "\n",
    "# You can interrupt this cell's training loop at any time, and visualize the\n",
    "# intermediate results by running the next cell (below). You can then resume\n",
    "# training by simply executing this cell again.\n",
    "\n",
    "# How much time between logging and printing the current results.\n",
    "log_every_seconds = 20\n",
    "\n",
    "print(\"# (iteration number), T (elapsed seconds), \"\n",
    "      \"Ltr (training loss), Lge (test/generalization loss), \"\n",
    "      \"Ctr (training fraction nodes/edges labeled correctly), \"\n",
    "      \"Str (training fraction examples solved correctly), \"\n",
    "      \"Cge (test/generalization fraction nodes/edges labeled correctly), \"\n",
    "      \"Sge (test/generalization fraction examples solved correctly)\")\n",
    "\n",
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "for iteration in range(last_iteration, num_training_iterations):\n",
    "    last_iteration = iteration\n",
    "    feed_dict, _ = create_feed_dict(batch_size_tr, input_ph, target_ph)\n",
    "    train_values = sess.run({\n",
    "        \"step\": step_op,\n",
    "        \"target\": target_ph,\n",
    "        \"loss\": loss_op_tr,\n",
    "        \"outputs\": output_ops_tr\n",
    "    }, feed_dict=feed_dict)\n",
    "    the_time = time.time()\n",
    "    elapsed_since_last_log = the_time - last_log_time\n",
    "    if elapsed_since_last_log > log_every_seconds:\n",
    "        last_log_time = the_time\n",
    "        feed_dict, raw_graphs = create_feed_dict(batch_size_ge, input_ph, target_ph)\n",
    "        test_values = sess.run({\n",
    "            \"target\": target_ph,\n",
    "            \"loss\": loss_op_ge,\n",
    "            \"outputs\": output_ops_ge\n",
    "        },\n",
    "                           feed_dict=feed_dict)\n",
    "        # added use_nodes=False in compute_accuracy\n",
    "        correct_tr, solved_tr = compute_accuracy(\n",
    "            train_values[\"target\"], train_values[\"outputs\"][-1], use_nodes=False, use_edges=True)\n",
    "        correct_ge, solved_ge = compute_accuracy(\n",
    "            test_values[\"target\"], test_values[\"outputs\"][-1], use_nodes=False, use_edges=True)\n",
    "        elapsed = time.time() - start_time\n",
    "        losses_tr.append(train_values[\"loss\"])\n",
    "        corrects_tr.append(correct_tr)\n",
    "        solveds_tr.append(solved_tr)\n",
    "        losses_ge.append(test_values[\"loss\"])\n",
    "        corrects_ge.append(correct_ge)\n",
    "        solveds_ge.append(solved_ge)\n",
    "        logged_iterations.append(iteration)\n",
    "        print(\"# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Ctr {:.4f}, Str\"\n",
    "              \" {:.4f}, Cge {:.4f}, Sge {:.4f}\".format(\n",
    "                  iteration, elapsed, train_values[\"loss\"], test_values[\"loss\"],\n",
    "                  correct_tr, solved_tr, correct_ge, solved_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
